#+TITLE:     Visualizing and Understanding Recurrent Networks
#+AUTHOR:    mingzailao
#+EMAIL:     mingzailao@126.com
#+DATE:      2016-9-11
#+KEYWORDS:  Deep Learning, AI, Reinforcement Learning
#+LANGUAGE:  en


#+STARTUP: beamer
#+STARTUP: oddeven

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]

#+BEAMER_THEME: Darmstadt

#+OPTIONS:   H:2 toc:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)

* EXPERIMENTAL SETUP
** RECURRENT NEURAL NETWORK MODELS
*** Basic Setting
- $h_t^0=x_t$ 
- $\{h_t^L\}$ is used to predict an output vector $y_t$
- all $h\in \mathbb{R}^n$
*** Vanilla Recurrent Neural Network (RNN)
\begin{equation*}
h_t^l=tanh W^l 
\left({
\begin{array}{c}
h_t^{l-1}\\
h_{t-1}^l
\end{array}
}
\right)
\end{equation*}

** RECURRENT NEURAL NETWORK MODELS
*** Long Short-Term Memory (LSTM)
\begin{equation*}
\left({
\begin{array}{c}
i\\
f\\
o\\
g
\end{array}
}\right)
=
\left({
\begin{array}{c}
sigm\\
sigm\\
sigm\\
tanh
\end{array}
}\right)
W^l
\left({
\begin{array}{c}
h_t^{l-1}\\
h_{t-1}^l
\end{array}
}\right)
\end{equation*}
\begin{eqnarray*}
 c_t^l& =& f\odotc_{t-1}^{l}+i\odot g\\
 h_t^l&=&o\odot tanh(c_t^l)
\end{eqnarray*}
** RECURRENT NEURAL NETWORK MODELS
*** Gated Recurrent Unit (GRU)
\begin{equation*}
\left(
{
\begin{array}{c}
r\\
c
\end{array}
}
\right)=
\left(
{
\begin{array}{c}
sigm\\
sigm
\end{array}
}
\right)
W_r^l
\left(
{
\begin{array}{c}
h_t^{l-1}\\
h_{t-1}^l
\end{array}
}
\right)
\end{equation*}
\begin{eqnarray*}
\label{eq:4}
\tilde{h}_t^l & = & tanh(W_x^lh_t^{l-1}+W_g^l(r\odot h_{t-1}^l))\\
h_t^l&=&(1-z)\odot h_{t-1}^l+z\odot \tilde{h}_t^l
\end{eqnarray*}
** CHARACTER-LEVEL LANGUAGE MODELING
*** SETUP
- Input : a sequence of characters;
- Output : the next character for each character in the sequence;
- Between $h_t^{L}\to y$ : Softmax Classifier;
- Encoding : assuming a fixed vocabulary of K characters, we encode all characters with K-dimensional 1-of-K vectors(OneHot);
- Loss : cross-entropy loss;
** CHARACTER-LEVEL LANGUAGE MODELING
*** OPTIMIZATION
- Initialization : uniformly in range $[-0.08,0.08]$;
- Batch size : 100;
- Optimizier : RMSProp;
- Timestep : 100; 
- Epochs : 50; 
- Learning rate decay : after 10 epochs with decay rate 0.95 after each additional epoch; 
- Learning rate : $2\times 10^{-3}$
