#+TITLE:     Generative Adversarial Parallelization
#+AUTHOR:    mingzailao
#+EMAIL:     mingzailao@126.com
#+DATE:      <2016-11-22 Tue>
#+KEYWORDS:  Deep Learning
#+LANGUAGE:  en

#+STARTUP: beamer
#+STARTUP: oddeven
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+LATEX_HEADER: \usepackage{xeCJK}
#+LATEX_HEADER: \setCJKmainfont[BoldFont=STZhongsong, ItalicFont=STKaiti]{STSong}
#+LATEX_HEADER: \setCJKsansfont[BoldFont=STHeiti]{STXihei}
#+LATEX_HEADER: \setCJKmonofont{STFangsong}

#+BEAMER_THEME: Madrid
#+OPTIONS:   H:2 toc:t
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)

* GANs
** GANs
*** min-max
\begin{equation}
\label{eq:1}
\min_{G}\max_{D}\mathbb{E}_{\mathbf{x}\sim P_{data}(\mathbf{x})}[\log D(\mathbf{x})]+\mathbb{E}_{\mathbf{z}\sim P_z(\mathbf{z})}[\log (1-D(G(\mathbf{z})))]
\end{equation}
** GANs
*** Empirical Observations
The reality of training GANs is quite different from the ideal case due to the following reasons
1. the discriminative and the generative networks are bounded by a finite number of parameters, which limits their modeling capacity.
2. Partically speaking, the second term of the object function in Eq ref:eq:1 is a bottleneck early on in training, where the discriminator can perfectly distinguish the noisy sample coming from the generator. The argument of the log saturates and gradient will not flow to the generator.
3. The GAN objective function is known to be non-convex and it is defined over a high-dimensional space . That often results in failure of gradient-based training to converge.

** GANs
*** First issue
For this issue, it comes from the nature of the modelling problem, by the empirical observations, we can adopt parameter-efficient convolutional architectures.
*** Second issue 
The second issue is typically addressed by inverting the generator's minimization into the maximization formulation in Equation ref:eq:1  accordingly,
\begin{equation}
\label{eq:2}
\min_{G}\log(1-D(G(\mathbf{z})))\rightarrow\max_{G}\log (D(G(\mathbf{z})))
\end{equation}
This provides better gradient flow in the earlier stages of training cite:NIPS2014_5423
** GANs
*** Third issue
Empirically, convergence of the learning curve does not correspond to improved quality of samples coming from the GAN. 
Gradient-based optimization methods are only guaranteed to converge to a Nash 
Equilibrium for convex functions, whereas the loss surface of the neural networks 
used in GANs are highly non-convex and there is no guarantee that a Nash 
Equilibrium even exists.
* Paralletizing Generative Adversarial Networks(GAP)
** Paralletizing Generative Adversarial Networks
*** Generative Adversarial Metric
The core concept of the Generative Adversarial Metric cite:im16:gener 
is to swap one discriminator (generator) with the other discriminator (generator)
during the test phase.

The GAM concept can easily be extended from evaluation to the training phase.
** Paralletizing Generative Adversarial Networks

#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:13:05
[[file:Paralletizing Generative Adversarial Networks/screenshot_2016-11-22_20-13-05.png]]

** Paralletizing Generative Adversarial Networks
*** Keys
1. trains multiple GANs simultaneously.
2. every GAN do not shared parameters.
3. randomly swapping different discriminators(generators) every $K$ updates.
4. after training multiple GANS, select the best one based on the GAM
** Paralletizing Generative Adversarial Networks

#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:19:44
[[file:Paralletizing Generative Adversarial Networks/screenshot_2016-11-22_20-19-44.png]]
** Paralletizing Generative Adversarial Networks
*** My notes
后面是巴拉巴拉的make sense的东东，我的大致理解就是 一个好的拳击手是怎样成长的呢？ 陪练需要不断的
更换
* Experiments
** Experiments
*** Two GAN-variants
1. DCGAN
2. GRAN
** Experiments

#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:41:53
[[file:Experiments/screenshot_2016-11-22_20-41-53.png]]

** Experiments
#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:42:17
[[file:Experiments/screenshot_2016-11-22_20-42-17.png]]

** Experiments

#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:44:38
[[file:Experiments/screenshot_2016-11-22_20-44-38.png]]
** Experiments

#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:45:42
[[file:Experiments/screenshot_2016-11-22_20-45-42.png]]

** Experiments

#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:51:35
[[file:Experiments/screenshot_2016-11-22_20-51-35.png]]

** Experiments

#+DOWNLOADED: /tmp/screenshot.png @ 2016-11-22 20:52:00
[[file:Experiments/screenshot_2016-11-22_20-52-00.png]]

* Reference

  \bibliographystyle{plain}

  bibliography:~/PAPERS/BibTex/mingzailao.bib
